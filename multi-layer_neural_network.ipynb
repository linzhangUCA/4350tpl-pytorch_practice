{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Neural Network\n",
    "\n",
    "**Due: Mondy, 10/10/2022, 2:15 PM**\n",
    "\n",
    "Welcome to your fourth assignment. You will build a multi-layer neural network in this assignment.\n",
    "\n",
    "Contents:\n",
    "\n",
    "1. (10%) Exercise 2.1: Parameter initialization\n",
    "2. (5%) Exercise 2.2: Linear transformation\n",
    "3. (20%) Exercise 2.4: Forward propagation\n",
    "4. (10%) Exerise 3.1: Derivatives of activation functions\n",
    "5. (30%) Exercise 3.2: Gradients computation\n",
    "6. (5%) Exercise 3.3: Gradient descent\n",
    "7. (20%) Exercise 4: Training\n",
    "Instructions:\n",
    "\n",
    "- The code between the ### START CODE HERE ### and ### END CODE HERE ### comments will be graded.\n",
    "- **Change variable names at your own risk. Make sure you understand what you are doing.**\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly asked to do so.\n",
    "\n",
    "**You will learn:**\n",
    "- Usage of Rectified Linear Unit (ReLU) activation function.\n",
    "- Generalize number and dimension of the hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "**NOTE: math representations of forward and backward propogation has been updated. Please use the equations in the [updated slides](https://linzhanguca.github.io/_docs/applied_deep_learning-2022/0921/nn_p2.pdf) or the follows if you prefer no transopose in the forward propagation.**\n",
    "\n",
    "To build your neural network, you will complete several \"helper functions\". These helper functions will be used to realize the forward and backward propagation when training a K-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will:\n",
    "\n",
    "- Initialize the parameters for K-layer neural network.\n",
    "- Implement the forward propagation. \n",
    "     - Compute linear transformation $\\mathbf{Z}^{[k]} = \\mathbf{X}^{[k-1]} \\cdot \\mathbf{W}^{[k]} + \\mathbf{b}^{[k]}$.\n",
    "     - Compute activation: $X^{[k]} = g(\\mathbf{Z}^{[k]})$.\n",
    "     - Stack the \"linear transfortmation\" and \"activation\" to compute predictions in the final layer.\n",
    "- Compute the cross entropy loss: \n",
    "    $$\\mathcal{L(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{M}\\sum_{i=1}^M (-\\mathbf{y}log(\\hat{\\mathbf{y}}) - (1 - \\mathbf{y})log(1 - \\hat{\\mathbf{y}}))}$$\n",
    "- Compute gradients of the parameters for backward propagation.\n",
    "    $$d\\mathbf{Z}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{Z}^{[k]}}} = d\\mathbf{X}^{[k]} * g'^{[k]}(\\mathbf{Z}^{[k]})$$\n",
    "    $$d\\mathbf{W}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{W}^{[k]}}} = \\frac{1}{M}\\mathbf{X}^{[k-1]\\mathbf{T}} \\cdot d\\mathbf{Z}^{[k]}$$\n",
    "    $$d\\mathbf{b}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{b}^{[k]}}} = \\frac{1}{M} \\Sigma d\\mathbf{Z}^{[k]}$$\n",
    "    $$d\\mathbf{X}^{[k-1]} = \\frac{\\partial{J}}{\\partial{\\mathbf{X}^{[k-1]}}} = d\\mathbf{Z}^{[k]} \\cdot \\mathbf{W}^{[k]\\mathbf{T}}$$\n",
    "- Update the parameters using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Prepare Dataset\n",
    "\n",
    "First, let's import libraries that are neccissary. You may need to open a terminal, run `pip install opencv-python` to be able to use [OpenCV](https://opencv.org/) library: `import cv2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from utils import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(4350)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset. You can view any image by giving a valid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y, classes = load_data()\n",
    "# show a picture from training dataset\n",
    "index = 10\n",
    "plt.imshow(train_X[index])\n",
    "print(\n",
    "    f\"y = {train_y[index, 0]}, it's a {classes[train_y[index, 0]].decode('utf-8')} picture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data, so that all the images are flattened and every pixel is standardized to a value in the range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset\n",
    "M_train = train_X.shape[0]\n",
    "M_test = test_X.shape[0]\n",
    "image_size = train_X.shape[1:]\n",
    "print(f\"Number of training examples: m_train = {M_train}\")\n",
    "print(f\"Number of testing examples: m_test = {M_test}\")\n",
    "print(f\"Size of each picture: {image_size}\")\n",
    "\n",
    "# Flatten and standardize\n",
    "train_X_flatten = train_X.reshape(M_train, -1)\n",
    "test_X_flatten = test_X.reshape(M_test, -1)\n",
    "train_X = train_X_flatten / 255.0\n",
    "test_X = test_X_flatten / 255.0\n",
    "print(f\"train_X's shape: {train_X.shape}\")\n",
    "print(f\"test_X's shape: {test_X.shape}\")\n",
    "print(f\"train_y's shape: {train_y.shape}\")\n",
    "print(f\"test_y's shape: {test_y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Forward Propagation\n",
    "When training a neural network, you'll perform parameter initialization, linear transform, activation, loss computation for the forward propagation.\n",
    "\n",
    "### 2.1 - Initialize Parameters for a K-layer neural network\n",
    "For a K-layer neural network model, the structure of the parameters is determined by the dimension of the input data and dimensions of all the layers (hidden layers and output layer). \n",
    "\n",
    "#### **(10%) Exercise 2.1**: Parameter initialization\n",
    "Complete `init_params()` function to realize parameter initialization. Input data can be represented as a matrix: `[number of examples, dimension of feature]`. Given the input features/variables dimension: `input_dim` and the dimensions of all the layers (hidden layers and output layer): `[1, 2, ..., k, ..., K]`, generate randomized parameters to represent the connections between the layers. Store the initialized parameters in a python dictionary.\n",
    "\n",
    "> Hint: `np.random.randn(shape)*0.01` can be used to initialize the weights, and `np.zeros(shape)` can be used to intialize the bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(input_dim, layers_dims):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    input_dim -- scalar, input feature dimension (X.shape[1])\n",
    "    layers_dims -- list, dimensions of all layers\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionary, contains parameters:\n",
    "        Wk -- numpy array, weight matrix connects (k-1)th layer and kth layer\n",
    "        bk -- numpy array, bias vector connects (k-1)th layer and kth layer\n",
    "    \"\"\"\n",
    "\n",
    "    params = {}\n",
    "    dims = [input_dim] + layers_dims\n",
    "    for i in range(1, len(dims)):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        params['W' + str(i)] = None\n",
    "        params['b' + str(i)] = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert params['W' + str(i)].shape == (dims[i - 1], dims[i])\n",
    "        assert params['b' + str(i)].shape == (1, dims[i])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# test\n",
    "np.random.seed(4350)\n",
    "parameters = init_params(8, [4, 2, 1])\n",
    "print(f\"W1 = {parameters['W1']}\")\n",
    "print(f\"b1 = {parameters['b1']}\")\n",
    "print(f\"W2 = {parameters['W2']}\")\n",
    "print(f\"b2 = {parameters['b2']}\")\n",
    "print(f\"W3 = {parameters['W3']}\")\n",
    "print(f\"b3 = {parameters['b3']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "```console\n",
    "W1 = [[-0.0097866   0.00059012 -0.01427151 -0.00276329]\n",
    " [-0.00452002 -0.00642133  0.00224894  0.00107208]\n",
    " [-0.00669298 -0.0005123   0.02440584 -0.00889387]\n",
    " [ 0.00033471 -0.00492237 -0.01517072 -0.00901276]\n",
    " [ 0.00409083 -0.01204107  0.00813648  0.01565679]\n",
    " [ 0.00361836  0.01152207 -0.00198383  0.01222088]\n",
    " [ 0.0094327   0.00909426  0.02394542  0.02829161]\n",
    " [-0.0018956  -0.00823652 -0.00024429 -0.01177393]]\n",
    "b1 = [[0. 0. 0. 0.]]\n",
    "W2 = [[ 0.00214078 -0.00504377]\n",
    " [ 0.02233779  0.00344896]\n",
    " [ 0.00667291 -0.0050118 ]\n",
    " [-0.00957994 -0.0052361 ]]\n",
    "b2 = [[0. 0.]]\n",
    "W3 = [[ 0.00593265]\n",
    " [-0.00457577]]\n",
    "b3 = [[0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Linear Transformation\n",
    "Use the following equation to perform linear transformation:\n",
    "$$\\mathbf{Z}^{[k]} = \\mathbf{X}^{[k-1]} \\cdot \\mathbf{W}^{[k]} + \\mathbf{b}^{[k]}$$\n",
    "\n",
    "#### **(5%) Exercise 2.2**: Linear transformation.\n",
    "Complete `linear()` function to transoform the features in the previous layer (`X_j`) into the non-activated features in the next layer (`Z_k`) using parameters: `W_k` and `b_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(X_j, W_k, b_k):\n",
    "    \"\"\"\n",
    "    linear transformation converts neurons in previous layer to neuron in current layer.\n",
    "\n",
    "    Arguments:\n",
    "        X_j -- numpy array, feature matrix of the (k-1)th layer. shape: (number_of_examples, previous_layer_dimension)\n",
    "        W_k -- numpy array, weights matrix connects (k-1)th layer to kth layer. shape: (previous_layer_dimension, current_layer_dimension)\n",
    "        b_k -- numpy array, bias vector connects (k-1)th layer to kth layer. shape: (1, current_layer_dimension)\n",
    "\n",
    "    Returns:\n",
    "        Z_k -- numpy array, linear transformed (pre-activation) featues matrix. shape: (number_of_examples, current_layer_dimension ) \n",
    "        cache -- dictionary {'X_j', 'W_k', 'b_k', 'Z_k'}, useful intermediate results for later usage.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    Z_k = np.dot(X_j, W_k) + b_k\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert Z_k.shape == (X_j.shape[0], W_k.shape[1])\n",
    "    cache = {\n",
    "        'X_j': X_j,\n",
    "        'W_k': W_k,\n",
    "        'b_k': b_k,\n",
    "        'Z_k': Z_k,\n",
    "    }\n",
    "\n",
    "    return Z_k, cache\n",
    "\n",
    "# test\n",
    "np.random.seed(4350)\n",
    "X = np.random.randn(3, 4)\n",
    "W = np.random.randn(4, 2)\n",
    "b = np.random.randn(1, 2)\n",
    "Z, cache = linear(X, W, b)\n",
    "print(f\"pre-activation features: {Z}\")\n",
    "print(f\"linear cache: {cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "```console\n",
    "pre-activation features: [[-0.5691033   2.86655348]\n",
    " [ 1.50009864  1.85049448]\n",
    " [ 0.69190761 -2.80338728]]\n",
    "linear cache: {'X_j': array([[-0.97866006,  0.05901167, -1.42715059, -0.27632879],\n",
    "       [-0.45200202, -0.64213329,  0.22489383,  0.10720757],\n",
    "       [-0.66929759, -0.05123023,  2.44058435, -0.88938706]]), 'W_k': array([[ 0.03347135, -0.49223651],\n",
    "       [-1.51707154, -0.9012758 ],\n",
    "       [ 0.40908305, -1.20410713],\n",
    "       [ 0.81364786,  1.56567891]]), 'b_k': array([[0.36183616, 1.15220702]]), 'Z_k': array([[-0.5691033 ,  2.86655348],\n",
    "       [ 1.50009864,  1.85049448],\n",
    "       [ 0.69190761, -2.80338728]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Activation Functions\n",
    "Activation functions add non-linear transformations toward the features. \n",
    "- Sigmoid\n",
    "$$y = \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "- Hyperbolic Tangent\n",
    "$$y = tanh(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}$$\n",
    "- Rectified Linear Unit\n",
    "$$y = ReLU(x) = \n",
    "    \\begin{cases}\n",
    "        0   & x \\leq 0 \\\\\n",
    "        x   & x > 0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Linearly transformed features will be non-linearly activated by:\n",
    "$$X^{[k]} = g(\\mathbf{Z}^{[k]})$$\n",
    "All the activation functions are either pre-built in `utils.py` or included in NumPy library, you don't have to do anything in this step. However, you can run the following code block to observe how different activation functions affect the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4350)\n",
    "Z = np.random.randn(1,4)\n",
    "print(f\"\\noriginal feature: {Z}, \\nsigmoid activated feature: {sigmoid(Z)} \\ntanh activated feature: {np.tanh(Z)} \\nrelu activated feature: {relu(Z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Stack Linear and Activation \n",
    "Now, you can stack linear transformations and activations to implement any layer neural network's forward propagation.\n",
    "\n",
    "#### **(20%) Exercise 2.4: Forward propagation**\n",
    "Complete `forward()` function to implement the $K$-layer Neural Network. You will need to perform `linear()` then use any activation functions $K-1$ times, then follows the last `linear()` transformation followed by the `sigmoid()` activation.\n",
    "You'll get a column vector, `yhat` with shape: `(number of example, 1)` in the end. \n",
    "\n",
    "> Tips:\n",
    "- > Use the functions you had previously written \n",
    "- > Use a for loop to repeat `z=linear(x) -> x=relu(z)` or `z=linear(x) -> x=np.tanh(z)` or `z=linear(x) -> x=sigmoid(z)` (K-1) times\n",
    "- > Don't forget to keep track of the intermediate results in each layer. You can use a nested dictionary: `caches` to store `cache` from each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(inputs, params, activation='relu'):\n",
    "    \"\"\"\n",
    "    forward function loops linear and activation to tranform the original features into the probability of the predictions.\n",
    "\n",
    "    Arguments:\n",
    "        inputs -- numpy array, original feature matrix. shape: (number_of_examples, original_feature_dimension)\n",
    "        params -- dictionary, stores weights and biases connects (k-1)th layer to kth layer.\n",
    "        activation -- string, activation type for the hidden layers (exclude the final layer).\n",
    "\n",
    "    Returns:\n",
    "        yhat -- numpy array, column vector of the predicted probability of the classes. shape: (number_of_examples, 1) \n",
    "        caches -- nested dictionary {'layer1: {cache}', ... , 'layerK': {cache}}, stores all the intermediate results in each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    num_layers = len(params) // 2\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for k in range(1, num_layers):\n",
    "        Z_k, cache = None\n",
    "        if activation=='relu':\n",
    "            X_k = None\n",
    "        elif activation=='tanh':\n",
    "            X_k = None\n",
    "        elif activation=='sigmoid':\n",
    "            X_k = None\n",
    "        else:\n",
    "            X_k = Z_k\n",
    "        caches['layer' + str(k)] = cache\n",
    "\n",
    "    Z_K, cache = None # final layer linear transform\n",
    "    yhat = None  # final layer activation \n",
    "    caches['layer' + str(num_layers)] = cache\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert yhat.shape == (inputs.shape[0], 1)\n",
    "\n",
    "    return yhat, caches\n",
    "\n",
    "\n",
    "\n",
    "# Test forward()\n",
    "X = np.array([\n",
    "        [-0.31178367, 0.72900392, 0.21782079, -0.8990918],\n",
    "        [-2.48678065, 0.91325152, 1.12706373, -1.51409323],\n",
    "        [1.63929108, -0.4298936, 2.63128056, 0.60182225],\n",
    "        [-0.33588161, 1.23773784, 0.11112817, 0.12915125],\n",
    "        [0.07612761, -0.15512816, 0.63422534, 0.810655],\n",
    "    ]).T\n",
    "parameters = {\n",
    "    'W1': np.array([\n",
    "        [0.35480861, 1.81259031, -1.3564758, -0.46363197, 0.82465384],\n",
    "        [-1.17643148, 1.56448966, 0.71270509, -0.1810066, 0.53419953],\n",
    "        [-0.58661296, -1.48185327, 0.85724762, 0.94309899, 0.11444143],\n",
    "        [-0.02195668, -2.12714455, -0.83440747, -0.46550831, 0.23371059]\n",
    "    ]).T,\n",
    "    'b1': np.array([[1.38503523, -0.51962709, -0.78015214, 0.95560959]]),\n",
    "    'W2': np.array([\n",
    "        [-0.12673638, -1.36861282, 1.21848065, -0.85750144],\n",
    "        [-0.56147088, -1.0335199, 0.35877096, 1.07368134],\n",
    "        [-0.37550472, 0.39636757, -0.47144628, 2.33660781]\n",
    "    ]).T,\n",
    "    'b2': np.array([[1.50278553, -0.59545972, 0.52834106]]),\n",
    "    'W3': np.array([[0.9398248, 0.42628539, -0.75815703]]).T,\n",
    "    'b3': np.array([[-0.16236698]])}\n",
    "y, caches = forward(X, parameters)\n",
    "print(f\"preds = {y}\")\n",
    "print(f\"layer2 cache = {caches['layer2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```console\n",
    "preds = [[0.03921668]\n",
    " [0.70498921]\n",
    " [0.19734387]\n",
    " [0.04728177]]\n",
    "layer2 cache = {'X_j': array([[0.        , 0.        , 4.18500916, 5.05850802],\n",
    "       [3.18040135, 0.        , 0.        , 0.        ],\n",
    "       [0.4074501 , 3.18141622, 0.        , 0.        ],\n",
    "       [0.        , 0.        , 2.72141639, 3.82321852]]), 'W_k': array([[-0.12673638, -0.56147088, -0.37550472],\n",
    "       [-1.36861282, -1.0335199 ,  0.39636757],\n",
    "       [ 1.21848065,  0.35877096, -0.47144628],\n",
    "       [-0.85750144,  1.07368134,  2.33660781]]), 'b_k': array([[ 1.50278553, -0.59545972,  0.52834106]]), 'Z_k': array([[ 2.2644603 ,  6.3372257 , 10.3750834 ],\n",
    "       [ 1.09971298, -2.38116247, -0.66591466],\n",
    "       [-2.90298025, -4.11228806,  1.63635184],\n",
    "       [ 1.54036335,  4.48582383,  8.17870168]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Loss Function\n",
    "\n",
    "In modern deep learning community, cost and loss is used interchangebly. People no longer distinguish the example-wise loss and the total cost. It is convention that using loss function to represent the cost function we were using. You can use `loss_fn()` which is given below to compute the cross entropy loss and monitor the training procedure later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    loss = np.mean(-labels * np.log(preds) - (1-labels) * np.log(1-preds))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Backward Propagation\n",
    "\n",
    "Back propagation is used to calculate the gradient of the loss function with respect to the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Derivatives of Activation Functions\n",
    "The derivative of the activation function is essential to compute the gradients.  \n",
    "#### **(10%) Exercise 3.1: Derivatives of activation functions**\n",
    "- Derivative of sigmoid function: \n",
    "    $$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "- Derivative of hyperbolic tangent function: \n",
    "    $$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "- Derivative of relu function: \n",
    "    $$ReLU'(x) = \n",
    "        \\begin{cases}\n",
    "            0   & x \\leq 0 \\\\\n",
    "            1   & x > 0\n",
    "        \\end{cases}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# sigmoid derivative\n",
    "def d_sigmoid(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    dydx = None\n",
    "\n",
    "    return dydx\n",
    "\n",
    "# tanh derivative\n",
    "def d_tanh(x):\n",
    "    y = np.tanh(x)\n",
    "    dydx = None\n",
    "\n",
    "    return dydx\n",
    "\n",
    "# relu derivative\n",
    "def d_relu(x):\n",
    "    dydx = None\n",
    "\n",
    "    return dydx\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Backward Loop\n",
    "Now, you can compute the gradients of the weights and biases from the last layer to the first layer using the intermediate results and the derivatives of the activation functions.\n",
    "\n",
    "#### **(30%) Exercise 3.2: Gradients computation**\n",
    "Complete `backward()` function. Loop the following equations to compute the gradients of the parameters.\n",
    "$$d\\mathbf{Z}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{Z}^{[k]}}} = d\\mathbf{X}^{[k]} * g'^{[k]}(\\mathbf{Z}^{[k]})$$\n",
    "$$d\\mathbf{W}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{W}^{[k]}}} = \\frac{1}{M}\\mathbf{X}^{[k-1]\\mathbf{T}} \\cdot d\\mathbf{Z}^{[k]}$$\n",
    "$$d\\mathbf{b}^{[k]} = \\frac{\\partial{J}}{\\partial{\\mathbf{b}^{[k]}}} = \\frac{1}{M} \\Sigma d\\mathbf{Z}^{[k]}$$\n",
    "$$d\\mathbf{X}^{[k-1]} = \\frac{\\partial{J}}{\\partial{\\mathbf{X}^{[k-1]}}} = d\\mathbf{Z}^{[k]} \\cdot \\mathbf{W}^{[k]\\mathbf{T}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(preds, labels, caches, activation='relu'):\n",
    "    \"\"\"\n",
    "    backward() function computes gradients of the parameters from the last layer all the way back to the first layer.\n",
    "\n",
    "    Arguments:\n",
    "        preds -- numpy array, probabilities of predictions. shape: (number_of_examples, 1)\n",
    "        labels -- numpy array, ground truth. shape: (number_of_examples, 1)\n",
    "        caches -- nested dictionary. Stores all the intermediate results in each layer.\n",
    "        activation -- string, activation type for the hidden layers (exclude the final layer).\n",
    "\n",
    "    Returns:\n",
    "        grads -- dictionary, gradients of the parameters in each layer \n",
    "    \"\"\"\n",
    "\n",
    "    assert preds.shape == labels.shape\n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    M = labels.shape[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    dX_k = None\n",
    "    for k in reversed(range(1, K + 1)):\n",
    "        if k == K:\n",
    "            dZ_k = None  # final layer activation derivative\n",
    "        else:\n",
    "            if activation=='relu':\n",
    "                dZ_k = None\n",
    "            elif activation=='tanh':\n",
    "                dZ_k = None\n",
    "            elif activation=='sigmoid':\n",
    "                dZ_k = None\n",
    "            else:\n",
    "                dZ_k = dX_k\n",
    "        dW_k = None\n",
    "        db_k = None\n",
    "        dX_j = None\n",
    "        dX_k = None\n",
    "        grads['dW' + str(k)] = None\n",
    "        grads['db' + str(k)] = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# Test backward()\n",
    "yhat = np.array([[1.78862847, 0.43650985]]).T\n",
    "y = np.array([[1, 0]]).T\n",
    "caches = {\n",
    "    'layer1': {\n",
    "        'X_j': np.array([\n",
    "            [0.09649747, -1.8634927],\n",
    "            [-0.2773882, -0.35475898],\n",
    "            [-0.08274148, -0.62700068],\n",
    "            [-0.04381817, -0.47721803]\n",
    "        ]).T,\n",
    "        'W_k': np.array([\n",
    "            [-1.31386475, 0.88462238, 0.88131804, 1.70957306],\n",
    "            [0.05003364, -0.40467741, -0.54535995, -1.54647732],\n",
    "            [0.98236743, -1.10106763, -1.18504653, -0.2056499]\n",
    "        ]).T,\n",
    "        'b_k': np.array([\n",
    "            [1.48614836],\n",
    "            [0.23671627],\n",
    "            [1.02378514]\n",
    "        ]).T,\n",
    "        'Z_k': np.array([\n",
    "            [-0.7129932, 0.62524497],\n",
    "            [-0.16051336, -0.76883635],\n",
    "            [-0.23003072, 0.74505627]\n",
    "        ]).T\n",
    "    },\n",
    "    'layer2': {\n",
    "        'X_j': np.array([\n",
    "            [1.97611078, -1.24412333],\n",
    "            [-0.62641691, -0.80376609],\n",
    "            [-2.41908317, -0.92379202]\n",
    "        ]).T,\n",
    "        'W_k': np.array([[-1.02387576, 1.12397796, -0.13191423]]).T,\n",
    "        'b_k': np.array([[-1.62328545]]),\n",
    "        'Z_k': np.array([[0.64667545, -0.35627076]]).T,\n",
    "    }\n",
    "}\n",
    "grads = backward(preds=yhat, labels=y, caches=caches)\n",
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "```console\n",
    "{'dW2': array([[-0.39202432],\n",
    "       [-0.13325855],\n",
    "       [-0.04601089]]), 'db2': array([[0.15187861]]), 'dW1': array([[0.41010002, 0.        , 0.05283652],\n",
    "       [0.07807203, 0.        , 0.01005865],\n",
    "       [0.13798444, 0.        , 0.01777766],\n",
    "       [0.10502167, 0.        , 0.0135308 ]]), 'db1': array([[-0.22007063,  0.        , -0.02835349]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "$$ W^{[k]} = W^{[k]} - \\alpha \\text{ } dW^{[k]} $$\n",
    "$$ b^{[k]} = b^{[k]} - \\alpha \\text{ } db^{[k]} $$\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them back to the parameters dictionary. \n",
    "\n",
    "#### **(5%) Exercise 3.3:** Gradient Descent\n",
    "Complete `update()` to update your parameters using gradient descent.\n",
    "> Hint: use `str()` to convert integers into strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    backward() function computes gradients of the parameters from the last layer all the way back to the first layer.\n",
    "\n",
    "    Arguments:\n",
    "        params -- dictionary, old parameters.\n",
    "        grads -- dictionary, gradients of the parameters in each layer \n",
    "        learning_rate -- scalar, controls the speed of training.\n",
    "\n",
    "    Returns:\n",
    "        params -- dictionary, updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) // 2\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for k in range(K):\n",
    "        params['W' + str(k + 1)] = None\n",
    "        params['b' + str(k + 1)] = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# test\n",
    "np.random.seed(4350)\n",
    "W1 = np.random.randn(4, 3)\n",
    "b1 = np.random.randn(1, 3)\n",
    "W2 = np.random.randn(3, 1)\n",
    "b2 = np.random.randn(1, 1)\n",
    "parameters = {\"W1\": W1,\n",
    "                \"b1\": b1,\n",
    "                \"W2\": W2,\n",
    "                \"b2\": b2}\n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(4, 3)\n",
    "db1 = np.random.randn(1, 3)\n",
    "dW2 = np.random.randn(3, 1)\n",
    "db2 = np.random.randn(1, 1)\n",
    "grads = {\"dW1\": dW1,\n",
    "            \"db1\": db1,\n",
    "            \"dW2\": dW2,\n",
    "            \"db2\": db2}\n",
    "\n",
    "parameters = update(parameters, grads, 0.1)\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```console\n",
    "W1 = [[-1.1575229   0.01536069 -1.43680034]\n",
    " [-0.08997952 -0.4242632  -0.60665739]\n",
    " [ 0.23316797  0.16990764 -0.66491577]\n",
    " [-0.00350843  2.57197082 -0.9778493 ]]\n",
    "b1 = [[-0.05466045 -0.66319382 -1.52207491]]\n",
    "W2 = [[-0.86080806]\n",
    " [ 0.46361905]\n",
    " [-1.0494594 ]]\n",
    "b2 = [[0.71541112]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train a K-layer Neural Network\n",
    "Put up together forward and backward propagation and train a neural network with any number of layers\n",
    "\n",
    "#### **(20%) Exercise 4.1:** Training\n",
    "Complete `train()` function. You can tune your model by using different `layers_dims`, `activation`, `learning_rate`, `num_iterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    inputs, labels, params, activation='relu', learning_rate=0.0075, num_iterations=3000, print_cost=False\n",
    "):\n",
    "    \"\"\"\n",
    "    train() function performs forward and backward propagation to obtain updated parameters.\n",
    "\n",
    "    Arguments:\n",
    "        inputs -- numpy array, original feature matrix. shape: (number_of_examples, original_feature_dimension)\n",
    "        labels -- numpy array, ground truth. shape: (number_of_examples, 1)\n",
    "        params -- dictionary, stores weights and biases connects (k-1)th layer to kth layer.\n",
    "        activation -- string, activation type for the hidden layers (exclude the final layer).\n",
    "        learning_rate -- scalar, controls the speed of training.\n",
    "        num_iterations -- scalar, numbers of training loops.\n",
    "        print_cost -- bool, to print averaged loss in every 100 iterations or not.\n",
    "\n",
    "    Returns:\n",
    "        params -- dictionary, updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    losses = []  # to keep track of the cost\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(1, num_iterations+1):\n",
    "        preds, caches = None\n",
    "        loss = None\n",
    "        grads = None\n",
    "        params = None\n",
    "        if print_cost and not (i % 100):\n",
    "            print(f\"Cost after iteration {i}: {np.squeeze(loss)}\")\n",
    "        losses.append(loss)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title('Learning rate =' + str(learning_rate))\n",
    "    # plt.show()\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# Training\n",
    "np.random.seed(4350)\n",
    "\n",
    "### START CODE HERE ###\n",
    "layers_dims = None\n",
    "params = None  # init params\n",
    "### END CODE HERE ###\n",
    "\n",
    "params = train(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    params,\n",
    "    activation='relu',\n",
    "    num_iterations=5000,\n",
    "    learning_rate=0.03,\n",
    "    # num_iterations=13500,\n",
    "    # learning_rate=0.01,\n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "train_preds, _ = forward(train_X, params)\n",
    "train_preds = train_preds > 0.5\n",
    "train_accuracy = np.sum(train_preds == train_y) / train_y.shape[0]\n",
    "print(f\"train_accuracy: {train_accuracy}\")\n",
    "test_preds, _ = forward(test_X, params)\n",
    "test_preds = test_preds > 0.5\n",
    "test_accuracy = np.sum(test_preds == test_y) / test_y.shape[0]\n",
    "print(f\"test_accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```console\n",
    "Cost after iteration 100: 0.6555767625124284\n",
    "Cost after iteration 200: 0.6467861650031772\n",
    "Cost after iteration 300: 0.644673251343426\n",
    "Cost after iteration 400: 0.6441493622996151\n",
    "Cost after iteration 500: 0.6440171212577572\n",
    "Cost after iteration 600: 0.6439833108092134\n",
    "Cost after iteration 700: 0.6439744892856368\n",
    "Cost after iteration 800: 0.6439720291615333\n",
    "Cost after iteration 900: 0.6439711781357069\n",
    "Cost after iteration 1000: 0.6439707173668276\n",
    "Cost after iteration 1100: 0.6439703266137381\n",
    "Cost after iteration 1200: 0.6439699125428286\n",
    "Cost after iteration 1300: 0.6439694437892591\n",
    "Cost after iteration 1400: 0.643968904782073\n",
    "Cost after iteration 1500: 0.6439682828411811\n",
    "Cost after iteration 1600: 0.643967564970407\n",
    "Cost after iteration 1700: 0.643966720541062\n",
    "Cost after iteration 1800: 0.6439657237669492\n",
    "Cost after iteration 1900: 0.6439645425002293\n",
    "Cost after iteration 2000: 0.6439631343005117\n",
    "Cost after iteration 2100: 0.6439614362611531\n",
    "Cost after iteration 2200: 0.6439593737815364\n",
    "Cost after iteration 2300: 0.6439567865212631\n",
    "Cost after iteration 2400: 0.6439534598397181\n",
    "Cost after iteration 2500: 0.6439492745389768\n",
    "...\n",
    "Cost after iteration 4700: 0.25480665125699675\n",
    "Cost after iteration 4800: 0.24463425857274818\n",
    "Cost after iteration 4900: 0.1242258171168041\n",
    "Cost after iteration 5000: 0.29679326419487606\n",
    "\n",
    "train_accuracy: 0.7607655502392344\n",
    "test_accuracy: 0.78\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the training accuracy is relatively low and the later stage of the training is very fluctuated. The loss has the trend of getting smaller. The test accuracy is relatively high (compare to 68% in assignment 2). These are the good signs. Can you adjust the hyperparameters to train a better model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Your Own Images\n",
    "You can upload new images to `/images` folder and test them using the model you've trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def predict(X, params):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a K-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    pred -- predicted classes for the given data X\n",
    "    \"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    yhat, caches = forward(X, params, activation='relu')\n",
    "    pred = (yhat > 0.5).astype(np.float32)\n",
    "        \n",
    "    return pred\n",
    "\n",
    "\n",
    "# Make \n",
    "file = \"snow_leopard.jpg\"   # change this to the name of your image file \n",
    "# preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + file\n",
    "im = cv2.imread(fname)\n",
    "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "imresize = cv2.resize(im_rgb, image_size[:-1])\n",
    "imfloat = imresize/255.\n",
    "imflatten = imfloat.reshape(1,-1)\n",
    "pred_cls = predict(imflatten, params)\n",
    "print(pred_cls)\n",
    "plt.imshow(imresize)\n",
    "print(f\"Your model predicts a {classes[int(np.squeeze(pred_cls))].decode('utf-8')} picture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congrats on finishing this assignment! "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
